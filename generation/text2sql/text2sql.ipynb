{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai 모델에 sql 생성 요청하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "    ### DDL :\n",
    "    {ddl}\n",
    "\n",
    "    ### Question :\n",
    "    {question}\n",
    "\n",
    "    ### SQL :\n",
    "    {query}\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 제한(rate limit)을 관리하면서 비동기적으로 요청을 보낼 수 있음\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def make_requests_for_gpt_evaluation(df, filename, dir='requests'):\n",
    "    if not Path(dir).exists():\n",
    "        Path(dir).mkdir(parents=True)\n",
    "    prompts = []\n",
    "    for idx, row in df.iterrows():\n",
    "        prompts.append(\"\"\"Based on below DDL and Question, evaluate gen_sql can resolve Question. \\\n",
    "                       If gen_sql and gt_sql do equal job, return \"yes\" else return \"no\". Output JSON Format: {\"resolve_yn\": \"\"}\"\"\"\\\n",
    "                        + f\"\"\"\n",
    "### DDL : {row['ddl']}\n",
    "### Question : {row['question']}\n",
    "### gt_sql : {row['gt_sql']}\n",
    "### gen_sql : {row['gen_sql']}\"\"\"\n",
    ")\n",
    "\n",
    "    jobs = [\n",
    "        {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_object\"\n",
    "                },\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    with open(Path(dir, filename), 'w') as f:\n",
    "        for job in jobs:\n",
    "            json_string = json.dumps(job)\n",
    "            f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# python api_request_parallel_processor.py \\\n",
    "#   --requests_filepath requests/gpt-4o-mini_requests.jsonl\n",
    "#   --save_filepath results/gpt-4o-mini_results.jsonl\n",
    "#   --request_url https://api.openai.com/v1/chat/completions\n",
    "#   --api_key $OPENAI_API_KEY\n",
    "#   --max_requests_per_minute 100\n",
    "#   --max_tokens_per_minute 100000\n",
    "#   --token_encoding_name cl100k_base\n",
    "#   --max_attempts 3\n",
    "#   --logging_level 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과를 csv 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def change_jsonl_to_csv(input_file, output_file, prompt_column=\"prompt\", response_column=\"response\"):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        for data in f:\n",
    "            prompts.append(json.loads(data)[0]['messages'][0]['content'])\n",
    "            responses.append(json.loads(data)[1]['choices'][0]['message']['content'])\n",
    "\n",
    "    df = pd.DataFrame({prompt_column: prompts, response_column: responses})\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### === 성능 평가 파이프라인 준비 완료 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습: 미세 조정 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 모델로 생성하기\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "def make_inference_pipeline(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return pipe\n",
    "\n",
    "model_id = \"beomi/Yi-Ko-6B\"\n",
    "hf_pipe = make_inference_pipeline(model_id)\n",
    "\n",
    "example = \"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL :\n",
    "CREATE TABLE players (\n",
    "    player_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    username VARCHAR(255) UNIQUE NOT NULL,\n",
    "    email VARCHAR(255) UNIQUE NOT NULL,\n",
    "    password_hash VARCHAR(255) NOT NULL,\n",
    "    date_joined DATETIME NOT NULL,\n",
    "    last_login DATETIME\n",
    ");\n",
    "\n",
    "### Question :\n",
    "사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요.\n",
    "\n",
    "### SQL :\n",
    "\"\"\"\n",
    "\n",
    "hf_pipe(example, do_sample=False, return_full_text=False, max_length=1024, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 모델 성능 측정\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "df = df.to_pandas()\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = make_prompt(row['context'], row['question'])\n",
    "    df.loc[idx, 'prompt'] = prompt\n",
    "\n",
    "# sql 생성\n",
    "gen_sqls = hf_pipe(df['prompt'].tolist(), do_sample=False,\n",
    "                   return_full_text=False, max_length=1024, truncation=True)\n",
    "gen_sqls = [x[0]['generated_text'] for x in gen_sqls]\n",
    "df['gen_sql'] = gen_sqls\n",
    "\n",
    "# 평가를 위한 requests.jsonl 생성\n",
    "eval_filepath = \"text2sql_evaluation.jsonl\"\n",
    "make_requests_for_gpt_evaluation(df, eval_filepath)\n",
    "\n",
    "# GPT-4 평가 수행\n",
    "!python api_request_parallel_processor.py \\\n",
    "    --requests_filepath requests/{eval_filepath} \\\n",
    "    --save_filepath results/{eval_filepath} \\\n",
    "    --request_url https://api.openai.com/v1/chat/completions \\\n",
    "    --api_key $OPENAI_API_KEY \\\n",
    "    --max_requests_per_minute 2500 \\\n",
    "    --max_tokens_per_minute 100000 \\\n",
    "    --token_encoding_name cl100k_base \\\n",
    "    --max_attempts 5 \\\n",
    "    --logging_level 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세 조정 수행\n",
    "# 학습 데이터 불러오기\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_sql = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['train']\n",
    "df_sql = df_sql.to_pandas()\n",
    "df_sql = df_sql.dropna().sample(frac=1, random_state=42)\n",
    "df_sql = df_sql.query(\"db_id != 1\") # 데이터셋에서 평가에 사용하기로 한 db_id 가 1인 데이터는 제거한다\n",
    "\n",
    "for idx, row in df_sql.iterrows():\n",
    "    df_sql.loc[idx, 'text'] = make_prompt(row['context'], row['question'], row['answer'])\n",
    "\n",
    "!mkdir data\n",
    "df_sql.to_csv('data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotrain-advanced 라이브러리를 사용해 지도 미세 조정을 수행함\n",
    "\n",
    "base_model = 'beomi/Yi-Ko-6B'\n",
    "finetuned_model = 'yi-ko-6b-text2sql'\n",
    "\n",
    "# !autotrain llm \\\n",
    "#  --train \\\n",
    "#  --model {base_model} \\\n",
    "#  --project-name {finetuned_model} \\\n",
    "#  --data-path data/ \\\n",
    "#  --text-column text \\\n",
    "#  --lr 2e-4 \\\n",
    "#  --batch-size 8 \\\n",
    "#  --epochs 1 \\\n",
    "#  --block-size 1024 \\\n",
    "#  --warmup-ratio 0.1 \\\n",
    "#  --lora-r 16 \\\n",
    "#  --lora-alpha 32 \\\n",
    "#  --lora-dropout 0.05 \\\n",
    "#  --weight-decay 0.01 \\\n",
    "#  --gradient-accumulation 8 \\\n",
    "#  --mixed-precision fp16 \\\n",
    "#  --use-peft \\\n",
    "#  --quantization int4 \\\n",
    "#  --trainer sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 어댑터 결합 및 허깅페이스 허브 업로드\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "model_name = start_model\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# LoRA와 기초 모델 파라미터 합치기\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map\n",
    "    )\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 토크나이저 설정   \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 허깅페이스 허브에 모델 및 토크나이저 저장\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세 조정한 모델로 예시 데이터에 대한 SQL 생성\n",
    "\n",
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "hf_pipe = make_inference_pipeline(model_id)\n",
    "\n",
    "hf_pipe(example, do_sample=False, return_full_text=False, max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세 조정한 모델 성능 측정\n",
    "# sql 생성 수행\n",
    "gen_sqls = hf_pipe(df['prompt'].tolist(),\n",
    "                   do_sample=False,\n",
    "                   return_full_text=False,\n",
    "                   max_length=1024,\n",
    "                   truncation=True)\n",
    "\n",
    "gen_sqls = [x[0]['generated_text'] for x in gen_sqls]\n",
    "df['gen_sql'] = gen_sqls\n",
    "\n",
    "# 평가를 위한 requests.jsonl 생성\n",
    "eval_filepath = \"text2sql_evaluation_finetuned.jsonl\"\n",
    "make_requests_for_gpt_evaluation(df, eval_filepath)\n",
    "\n",
    "# GPT-4 평가 수행\n",
    "# !python api_request_parallel_processor.py \\\n",
    "#     --requests_filepath requests/{eval_filepath} \\\n",
    "#     --save_filepath results/{eval_filepath} \\\n",
    "#     --request_url https://api.openai.com/v1/chat/completions \\\n",
    "#     --api_key $OPENAI_API_KEY \\\n",
    "#     --max_requests_per_minute 2500 \\\n",
    "#     --max_tokens_per_minute 100000 \\\n",
    "#     --token_encoding_name cl100k_base \\\n",
    "#     --max_attempts 5 \\\n",
    "#     --logging_level 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yi-Ko-6B 보다 두 배 더 큰 모델\n",
    "# base_model = 'beomi/OPEN-SOLAR-KO-10.7B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 연습문제\n",
    "\n",
    "## 1. 평가 데이터셋 늘리기\n",
    "## 2. 오류 분석과 데이터셋 추가 : 모델이 잘 생성하지 못하는 SQL 패턴이 있는지 분석, 학습 데이터셋 추가\n",
    "## 3. DPO 학습 활용 : GPT-4 가 생성한 결과를 chosen 세트, 기초 모델이나 미세 조정 모델이 생성한 결과를 rejected 세트로 사용\n",
    "## 4. 다른 모델 활용 : Code Llama ? (Code 생성에 특화된 모델인 만큼 코드와 유사한 SQL 생성도 잘 하지 않을까?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
