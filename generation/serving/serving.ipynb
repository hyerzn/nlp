{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오프라인 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 기본 배치 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "    ### DDL :\n",
    "    {ddl}\n",
    "\n",
    "    ### Question :\n",
    "    {question}\n",
    "\n",
    "    ### SQL :\n",
    "    {query}\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "dataset = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "dataset = dataset.to_pandas()\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    prompt = make_prompt(row['context'], row['question'])\n",
    "    dataset.loc[idx, 'prompt'] = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 크기에 따른 추론 시간 확인\n",
    "import time\n",
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    start_time = time.time()\n",
    "    hf_pipeline(dataset['prompt'].tolist(), max_new_tokens=128, batch_size=batch_size)\n",
    "    print(f\"{batch_size}: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "llm = LLM(model=model_id, dtype=torch.float16, max_model_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM을 활용한 오프라인 추론 시간 측정\n",
    "\n",
    "for max_num_seqs in [1, 2, 4, 8, 16, 32]:\n",
    "    start_time = time.time()\n",
    "    llm.llm_engine.scheduler_config.max_num_seqs = max_num_seqs\n",
    "    sampling_params = SamplingParams(temperature=1, top_p=1, max_tokens=128)\n",
    "    outputs = llm.generate(dataset['prompt'].tolist(), sampling_params)\n",
    "    print(f\"{max_num_seqs}: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 온라인 서빙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온라인 서빙을 위한 vLLM API 서버 실행\n",
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "--model shangrilar/yi-ko-6b-text2sql \\\n",
    "--host 127.0.0.1 \\\n",
    "--port 8888 \\\n",
    "--max-model-len 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 서버 실행 확인\n",
    "!curl http://localhost:8888/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 요청\n",
    "import json\n",
    "\n",
    "json_data = json.dumps(\n",
    "    {\n",
    "        \"model\": \"shangrilar/yi-ko-6b-text2sql\",\n",
    "        \"prompt\": dataset.loc[0, 'prompt'],\n",
    "        \"max_tokens\": 128,\n",
    "        \"temperature\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "!curl http://localhost:8888/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{json_data}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 클라이언트를 사용한 API 요청\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"\"\n",
    "openai_api_base = \"http://localhost:8888/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base\n",
    ")\n",
    "completion = client.completions.create(\n",
    "    model=\"shangrilar/yi-ko-6b-text2sql\",\n",
    "    prompt=dataset.loc[0, 'prompt'],\n",
    "    max_tokens=128\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
